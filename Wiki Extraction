#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
This script extracts all entries from the Simple English Wiktionary’s “All Pages” list.
It starts at the URL:
    https://simple.wiktionary.org/w/index.php?title=Special:AllPages&from=%21
and then iteratively follows the "Next page" links.
Each page’s entries are extracted from list items of the form:
    <li><a href="/wiki/Entry" title="Entry">Entry</a></li>
Finally, the entries are saved to "wiktionary_entries.txt".
"""

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin

# Base URL for the All Pages list.
START_URL = "https://simple.wiktionary.org/w/index.php?title=Special:AllPages&from=%21"
# Delay between page requests (seconds) to be polite.
DELAY = 1.0

def extract_entries(soup):
    """Extracts all entries from the page soup.
    
    Looks for <li><a ...> tags where the href starts with "/wiki/".
    Returns a list of entry names.
    """
    entries = []
    # Often the entries are within a container with id "mw-allpages-body"
    body = soup.find("div", id="mw-allpages-body")
    if not body:
        body = soup  # fallback to entire soup
    li_tags = body.find_all("li")
    for li in li_tags:
        a = li.find("a")
        if a and a.get("href", "").startswith("/wiki/"):
            entry = a.get_text(strip=True)
            entries.append(entry)
    return entries

def find_next_page(soup, current_url):
    """
    Finds the next page URL from the current page soup.
    The next page link is within a <div class="mw-allpages-nav"> and its text starts with "Next page".
    Returns the absolute URL if found, or None otherwise.
    """
    nav_div = soup.find("div", class_="mw-allpages-nav")
    if nav_div:
        # Look for an <a> whose text starts with "Next page"
        next_link = nav_div.find("a", string=lambda t: t and t.startswith("Next page"))
        if next_link and next_link.get("href"):
            next_url = urljoin(current_url, next_link["href"])
            return next_url
    return None

def main():
    all_entries = set()
    current_url = START_URL
    page_counter = 0

    while current_url:
        page_counter += 1
        print(f"Processing page {page_counter}: {current_url}")
        response = requests.get(current_url)
        if response.status_code != 200:
            print(f"Failed to retrieve {current_url} (status code {response.status_code})")
            break
        
        soup = BeautifulSoup(response.text, "html.parser")
        entries = extract_entries(soup)
        print(f"Found {len(entries)} entries on this page.")
        for entry in entries:
            all_entries.add(entry)
        
        next_url = find_next_page(soup, current_url)
        if next_url:
            current_url = next_url
            time.sleep(DELAY)  # be polite, pause between requests
        else:
            print("No next page found. Extraction complete.")
            break

    # Save the entries to a text file.
    sorted_entries = sorted(all_entries)
    with open("wiktionary_entries.txt", "w", encoding="utf-8") as f:
        for entry in sorted_entries:
            f.write(f"{entry}\n")
    
    print(f"Extracted {len(sorted_entries)} unique entries.")
    print("Entries saved to wiktionary_entries.txt")

if __name__ == "__main__":
    main()
